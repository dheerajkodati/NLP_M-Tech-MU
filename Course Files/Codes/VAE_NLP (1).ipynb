{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random"
      ],
      "metadata": {
        "id": "IrK2TAOQ5LCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy corpus (you can replace with a large dataset)\n",
        "sentences = [\n",
        "    \"i love machine learning\",\n",
        "    \"deep learning is powerful\",\n",
        "    \"nlp is fun\",\n",
        "    \"i enjoy coding\",\n",
        "    \"language models are awesome\",\n",
        "    \"i like natural language processing\",\n",
        "]"
      ],
      "metadata": {
        "id": "wgOHXgInJUn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Vocabulary & Tokenization\n",
        "class Vocab:\n",
        "    def __init__(self, texts):\n",
        "        tokens = set(word for sent in texts for word in sent.split())\n",
        "        self.word2idx = {w: i+2 for i, w in enumerate(tokens)}\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        self.word2idx['<unk>'] = 1\n",
        "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "\n",
        "    def encode(self, sent):\n",
        "        return [self.word2idx.get(word, 1) for word in sent.split()]\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return ' '.join([self.idx2word[i] for i in ids if i != 0])\n",
        "\n",
        "vocab = Vocab(sentences)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czZ_okRTJdq8",
        "outputId": "fe02a93f-ac3d-4b56-cde3-56883708531e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20\n",
            "<__main__.Vocab object at 0x78c58741ec90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sentences, vocab, max_len=8):\n",
        "        self.data = [vocab.encode(s) for s in sentences]\n",
        "        self.data = [s + [0]*(max_len - len(s)) for s in self.data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx], dtype=torch.long)\n",
        "\n",
        "dataset = TextDataset(sentences, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrxvUgqJJvz9",
        "outputId": "d5e2f84b-4f95-430c-afd5-d3c0e694b394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.TextDataset object at 0x78c587305f50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. VAE Model\n",
        "class TextVAE(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128, latent_dim=32):\n",
        "        super(TextVAE, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder_rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.hidden2mean = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.hidden2logv = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.latent2hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.decoder_rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.outputs2vocab = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def encode(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        _, h = self.encoder_rnn(emb)\n",
        "        h = h.squeeze(0)\n",
        "        return self.hidden2mean(h), self.hidden2logv(h)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode_stepwise(self, z, max_len=15, temperature=0.8, start_token=\"i\"):\n",
        "        h = self.latent2hidden(z).unsqueeze(0)\n",
        "        start_idx = vocab.word2idx.get(start_token, random.randint(2, vocab.vocab_size - 1))\n",
        "        inputs = torch.tensor([[start_idx]], dtype=torch.long).to(z.device)\n",
        "\n",
        "        outputs = []\n",
        "        prev_token = None\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            emb = self.embedding(inputs)\n",
        "            out, h = self.decoder_rnn(emb, h)\n",
        "            logits = self.outputs2vocab(out[:, -1, :])\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            next_token_id = next_token.item()\n",
        "\n",
        "            # Avoid repetition and pad\n",
        "            if next_token_id != prev_token and next_token_id != 0:\n",
        "                outputs.append(next_token_id)\n",
        "                prev_token = next_token_id\n",
        "\n",
        "            inputs = next_token\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_recon = self.decode_from_z(z, x.size(1))\n",
        "        return x_recon, mu, logvar\n",
        "\n",
        "    def decode_from_z(self, z, seq_len):\n",
        "        h = self.latent2hidden(z).unsqueeze(0)\n",
        "        inputs = torch.full((z.size(0), seq_len), vocab.word2idx['<pad>'], dtype=torch.long).to(z.device)\n",
        "        inputs[:, 0] = random.randint(2, vocab.vocab_size - 1)\n",
        "        emb = self.embedding(inputs)\n",
        "        out, _ = self.decoder_rnn(emb, h)\n",
        "        return self.outputs2vocab(out)"
      ],
      "metadata": {
        "id": "xPaKc3mJJm5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Loss Function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    recon_loss = F.cross_entropy(recon_x.view(-1, recon_x.size(-1)), x.view(-1), ignore_index=0)\n",
        "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
        "    return recon_loss + kl_div"
      ],
      "metadata": {
        "id": "1r3itEoJJj7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = TextVAE(vocab_size=vocab.vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = model(batch)\n",
        "        loss = vae_loss(recon, batch, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbMTW9LBJgYt",
        "outputId": "ae6a2518-56d6-4ba9-86ba-9c6655ffa7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 10.5910\n",
            "Epoch 2, Loss: 9.3555\n",
            "Epoch 3, Loss: 9.0976\n",
            "Epoch 4, Loss: 8.9725\n",
            "Epoch 5, Loss: 8.7550\n",
            "Epoch 6, Loss: 8.5339\n",
            "Epoch 7, Loss: 8.4723\n",
            "Epoch 8, Loss: 8.5010\n",
            "Epoch 9, Loss: 8.4817\n",
            "Epoch 10, Loss: 8.3999\n",
            "Epoch 11, Loss: 8.3621\n",
            "Epoch 12, Loss: 8.3269\n",
            "Epoch 13, Loss: 8.2855\n",
            "Epoch 14, Loss: 7.9773\n",
            "Epoch 15, Loss: 8.1695\n",
            "Epoch 16, Loss: 8.0114\n",
            "Epoch 17, Loss: 7.9640\n",
            "Epoch 18, Loss: 8.0304\n",
            "Epoch 19, Loss: 8.0251\n",
            "Epoch 20, Loss: 7.7483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Text Generation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(1, 32).to(device)\n",
        "    print(z)\n",
        "    output_ids = model.decode_stepwise(z, max_len=12, temperature=0.8, start_token=\"language\")\n",
        "    print(\"\\n📝 Generated Sentence:\\n\", vocab.decode(output_ids))\n"
      ],
      "metadata": {
        "id": "PyTAHigH5LFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae20374-8d8e-4b48-c333-10e5ca2b4dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.9235,  0.4659,  1.2726, -0.4064,  0.2897, -0.9690,  0.0390, -1.3075,\n",
            "         -0.0789,  1.8578, -0.7934, -1.2537,  0.9834,  0.1393, -0.9805, -0.6045,\n",
            "          1.4312,  0.0861,  1.0327, -0.1965, -1.7563, -0.4395,  0.2122, -1.0704,\n",
            "         -0.3338, -0.8653, -0.4654, -0.9062,  0.4435,  1.6028,  1.1854,  1.7066]])\n",
            "\n",
            "📝 Generated Sentence:\n",
            " processing are awesome like fun <unk> like fun powerful models i are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Sample sentence data\n",
        "sentences = [\n",
        "    \"i enjoy working on ai\",\n",
        "    \"machine learning is fun\",\n",
        "    \"deep learning helps a lot\",\n",
        "    \"natural language is powerful\",\n",
        "    \"models can learn patterns\"\n",
        "]\n",
        "\n",
        "# Vocabulary preparation\n",
        "class Vocab:\n",
        "    def __init__(self, sentences):\n",
        "        tokens = {\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"}\n",
        "        for sentence in sentences:\n",
        "            tokens.update(sentence.split())\n",
        "        self.word2idx = {w: i for i, w in enumerate(sorted(tokens))}\n",
        "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
        "        self.pad = self.word2idx[\"<pad>\"]\n",
        "        self.sos = self.word2idx[\"<sos>\"]\n",
        "        self.eos = self.word2idx[\"<eos>\"]\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "\n",
        "    def encode(self, sentence, max_len=10):\n",
        "        tokens = [self.word2idx.get(w, self.word2idx[\"<unk>\"]) for w in sentence.split()]\n",
        "        tokens = [self.sos] + tokens + [self.eos]\n",
        "        tokens += [self.pad] * (max_len - len(tokens))\n",
        "        return tokens[:max_len]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        words = []\n",
        "        for i in indices:\n",
        "            w = self.idx2word.get(i, \"<unk>\")\n",
        "            if w in {\"<pad>\", \"<sos>\", \"<eos>\"}: continue\n",
        "            words.append(w)\n",
        "        return \" \".join(words)\n",
        "\n",
        "vocab = Vocab(sentences)\n",
        "\n",
        "# Dataset\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences, vocab):\n",
        "        self.data = [vocab.encode(s) for s in sentences]\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.data[idx])\n",
        "        return x, x  # input, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "dataset = SentenceDataset(sentences, vocab)\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# VAE Modules\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, hidden_dim=128, latent_dim=32):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.hidden2mean = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.hidden2logv = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)\n",
        "        _, h = self.rnn(emb)\n",
        "        mean = self.hidden2mean(h.squeeze(0))\n",
        "        logv = self.hidden2logv(h.squeeze(0))\n",
        "        std = torch.exp(0.5 * logv)\n",
        "        z = mean + std * torch.randn_like(std)\n",
        "        return z, mean, logv\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, latent_dim=32, hidden_dim=128, max_len=10):\n",
        "        super().__init__()\n",
        "        self.latent2hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def forward(self, z, targets=None, teacher_forcing_ratio=0.5):\n",
        "        batch_size = z.size(0)\n",
        "        hidden = self.latent2hidden(z).unsqueeze(0)\n",
        "        input_token = torch.full((batch_size, 1), vocab.sos).to(z.device)\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(self.max_len):\n",
        "            emb = self.embed(input_token)\n",
        "            output, hidden = self.rnn(emb, hidden)\n",
        "            logits = self.fc(output)\n",
        "            outputs.append(logits)\n",
        "            top1 = logits.argmax(2)\n",
        "            input_token = top1 if (targets is None or torch.rand(1).item() > teacher_forcing_ratio) else targets[:, t].unsqueeze(1)\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "# VAE Model\n",
        "class SentenceVAE(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size)\n",
        "        self.decoder = Decoder(vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mean, logv = self.encoder(x)\n",
        "        outputs = self.decoder(z, targets=x)\n",
        "        return outputs, mean, logv\n",
        "\n",
        "# Loss Function\n",
        "def vae_loss(recon, target, mean, logv):\n",
        "    recon = recon.view(-1, vocab.vocab_size)\n",
        "    target = target.view(-1)\n",
        "    CE = F.cross_entropy(recon, target, ignore_index=vocab.pad)\n",
        "    KL = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp()) / mean.size(0)\n",
        "    return CE + KL\n",
        "\n",
        "# Training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentenceVAE(vocab.vocab_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        recon, mean, logv = model(x)\n",
        "        loss = vae_loss(recon, y, mean, logv)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Sentence Generation\n",
        "def reconstruct(sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor([vocab.encode(sentence)]).to(device)\n",
        "        z, _, _ = model.encoder(x)\n",
        "        outputs = model.decoder(z)\n",
        "        tokens = outputs.argmax(2).squeeze(0).tolist()\n",
        "        return vocab.decode(tokens)\n",
        "\n",
        "# Try some test sentences\n",
        "print(\"\\n--- Sentence Reconstruction ---\")\n",
        "for s in sentences:\n",
        "    print(f\"Input     : {s}\")\n",
        "    print(f\"Rewritten : {reconstruct(s)}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k0iBZcB5LIy",
        "outputId": "520f7277-e949-4e20-c2ca-909b9ce8d491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 4.1399\n",
            "Epoch 20, Loss: 2.8232\n",
            "Epoch 30, Loss: 2.0676\n",
            "Epoch 40, Loss: 2.4734\n",
            "Epoch 50, Loss: 3.9451\n",
            "\n",
            "--- Sentence Reconstruction ---\n",
            "Input     : i enjoy working on ai\n",
            "Rewritten : deep learning helps a lot\n",
            "\n",
            "Input     : machine learning is fun\n",
            "Rewritten : deep learning helps a lot\n",
            "\n",
            "Input     : deep learning helps a lot\n",
            "Rewritten : models can learn patterns\n",
            "\n",
            "Input     : natural language is powerful\n",
            "Rewritten : machine learning helps a lot\n",
            "\n",
            "Input     : models can learn patterns\n",
            "Rewritten : models can learn patterns\n",
            "\n"
          ]
        }
      ]
    }
  ]
}